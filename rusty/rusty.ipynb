{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39e96afa-cd10-4e0c-a67e-6361b13bcf96",
   "metadata": {},
   "source": [
    "Good day, my name is Mcpraise Leightong Okoi.\n",
    "\n",
    "Today, weâ€™ll be building a machine learning model that translates spoken English into written German. This will be a dynamic model, as weâ€™ll train it specifically on workplace conversations, allowing it to provide tailored translations based on the context of each conversation.\n",
    "\n",
    "Now, without further ado, letâ€™s begin! ðŸ¥‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb02309-f6cd-4caa-ab76-5a073065585b",
   "metadata": {},
   "source": [
    "#### importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e199a1-afa6-4157-9418-8cf7f3813da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a71a49-b087-45d9-80ca-cfaffe4eb328",
   "metadata": {},
   "source": [
    "#### Optional Step: Verify Compatibility of These Two Versions\n",
    "To ensure smooth performance, check that your versions of these two components are compatible. If they are not, proceed to upgrade both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef210c28-ab9d-4e99-af97-322131792751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# import accelerate\n",
    "\n",
    "# print(\"Transformers version:\", transformers.__version__)\n",
    "# print(\"Accelerate version:\", accelerate.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4952267-8aa9-4ad7-9b95-4d2b20ec1326",
   "metadata": {},
   "source": [
    "#### We will use the 'google-t5-small' model from Hugging Face and fine-tune it to align with our end goal.\n",
    "\n",
    "The code below downloads the model and saves it to your '.cache'. Weâ€™ll then use it to translate the dataset into its corresponding German text.\n",
    "\n",
    "The logic behind this approach is that, to achieve our end goal (i.e., for the model to recognize patterns and provide tailored translations), we need the data in both English and its corresponding German form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22f2aa8-5b68-4593-9dc4-7217eefc0623",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"t5-small\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"./datasets/Conversation.csv\")\n",
    "\n",
    "\n",
    "def translate_text(text):\n",
    "    input_text = f\"translate English to German: {text}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    output = model.generate(**inputs)\n",
    "    translated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# Translate the questions and answers\n",
    "df['questions_german'] = df['question'].apply(translate_text)\n",
    "df['answers_german'] = df['answer'].apply(translate_text)\n",
    "\n",
    "# Save the translated dataset\n",
    "df.to_csv(\"translated_dataset.csv\", index=False)\n",
    "print(\"Translation complete. Saved as 'translated_dataset.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ffbcc9-81d6-49d9-89e1-e425a33b5855",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "Now, weâ€™ll train the model on the translated_dataset, save it, and create directories for storing the results and logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7eb8e6-0083-4f1a-847f-88fc1e0619a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./datasets/translated_dataset.csv\")\n",
    "\n",
    "def prepare_data(df):\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        input_text = f\"translate English to German: {row['question']}\"\n",
    "        target_text = row['questions_german']\n",
    "        \n",
    "        input_texts.append(input_text)\n",
    "        target_texts.append(target_text)\n",
    "        \n",
    "    return input_texts, target_texts\n",
    "\n",
    "input_texts, target_texts = prepare_data(df)\n",
    "\n",
    "class TranslationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenizer, input_texts, target_texts, max_input_length=128, max_target_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_texts = input_texts\n",
    "        self.target_texts = target_texts\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_target_length = max_target_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_encoding = self.tokenizer(self.input_texts[idx], truncation=True, padding='max_length', max_length=self.max_input_length, return_tensors='pt')\n",
    "        target_encoding = self.tokenizer(self.target_texts[idx], truncation=True, padding='max_length', max_length=self.max_target_length, return_tensors='pt')\n",
    "\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_encoding['input_ids'].flatten(),\n",
    "            'attention_mask': input_encoding['attention_mask'].flatten(),\n",
    "            'labels': target_encoding['input_ids'].flatten()\n",
    "        }\n",
    "\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "train_dataset = TranslationDataset(tokenizer, input_texts, target_texts)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',            \n",
    "    num_train_epochs=3,                \n",
    "    per_device_train_batch_size=8,     \n",
    "    save_steps=10_000,                 \n",
    "    save_total_limit=2,                \n",
    "    logging_dir='./logs',              \n",
    "    logging_steps=200,                  \n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "model.save_pretrained(\"fine_tuned_t5\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_t5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f5524e-8548-43d8-bc9a-ff4baa4e5e76",
   "metadata": {},
   "source": [
    "#### Model Evaluation\n",
    "\n",
    "With the model already trained and saved, we can now load it to calculate metrics and assess its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eaa735-d3e5-40eb-8ca9-2bc6feaed677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# Load the tokenizer and fine-tuned model if not already loaded\n",
    "# (skip this step if already in memory)\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"fine_tuned_t5\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"fine_tuned_t5\")\n",
    "\n",
    "# Define your compute_metrics function if not defined earlier\n",
    "from datasets import load_metric\n",
    "bleu_metric = load_metric(\"bleu\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    bleu = bleu_metric.compute(predictions=[pred.split() for pred in decoded_preds],\n",
    "                               references=[[label.split()] for label in decoded_labels])\n",
    "    return {\"bleu\": bleu[\"bleu\"]}\n",
    "\n",
    "# Initialize the Trainer for evaluation\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,  # reuse previous training args if defined, or define new ones\n",
    "    eval_dataset=val_dataset,  # use your validation dataset\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Run evaluation only (this will not trigger training)\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "# Print out evaluation results\n",
    "print(\"Evaluation metrics:\", eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1e9d48-5808-4e7d-a6e2-7f7db7e14f60",
   "metadata": {},
   "source": [
    "#### Testing\n",
    "\n",
    "Now we test the model to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874c5714-b346-4c13-8a2b-7cf92b768339",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_name = \"fine_tuned_t5\"  # Directory where your fine-tuned model is saved\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"fine_tuned_t5\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"fine_tuned_t5\")\n",
    "\n",
    "# Function to translate English to German\n",
    "def translate(text):\n",
    "    input_text = f\"translate English to German: {text}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs)\n",
    "    translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# Test the model with workplace conversational phrases\n",
    "test_phrases = [\n",
    "    \"Could you please send me the latest report?\",\n",
    "    \"Let's schedule a meeting for tomorrow at 10 AM.\",\n",
    "    \"Thank you for your assistance with this project.\",\n",
    "    \"I'm looking forward to our collaboration on this task.\",\n",
    "]\n",
    "\n",
    "# Translate and print the results\n",
    "for phrase in test_phrases:\n",
    "    translation = translate(phrase)\n",
    "    print(f\"English: {phrase}\")\n",
    "    print(f\"German: {translation}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3defd9-513d-440b-9b01-2f9e17c85d0d",
   "metadata": {},
   "source": [
    "## THE END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b84e3ba-d2a7-474d-8b19-ed5b4c859e4e",
   "metadata": {},
   "source": [
    "Thank you for taking the time to review my work! Due to hardware limitations, I was unable to extend it to produce state-of-the-art results, but I hope you appreciated the concept.\n",
    "\n",
    "Stay safe and stay blessed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8214bd44-7b57-4646-8d1d-9260003c60cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
